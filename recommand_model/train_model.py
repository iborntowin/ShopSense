# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MCF8hT9l8uJS3zKdclAOyPZNFbGhZAsf
"""

"""
train_model.py
==============
Trains the hybrid recommendation system and saves model artifacts to models/ folder.

Generates 4 files:
1. recommendation_engine.pkl - Main recommendation matrices and data
2. feature_scaler.pkl - StandardScaler for customer features
3. kmeans_model.pkl - Trained K-Means clustering model
4. feature_info.pkl - Feature names and metadata
"""

import pandas as pd
import numpy as np
import pickle
import joblib
import os
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Create models directory if it doesn't exist
script_dir = os.path.dirname(os.path.abspath(__file__))
models_dir = os.path.join(script_dir, 'models')
os.makedirs(models_dir, exist_ok=True)

print("="*60)
print("HYBRID RECOMMENDATION SYSTEM - TRAINING")
print("="*60)

# ============================================================
# 1. DATA LOADING
# ============================================================

print("\nüì• Step 1: Loading dataset...")
# Get the directory of this script
import os
script_dir = os.path.dirname(os.path.abspath(__file__))
csv_path = os.path.join(script_dir, "shopping_trends_cleanfinal.csv")
df = pd.read_csv(csv_path)
print(f"‚úÖ Dataset loaded: {df.shape[0]} customers, {df.shape[1]} features")

# ============================================================
# 2. DATA PREPROCESSING
# ============================================================

print("\nüîß Step 2: Preprocessing data...")

# Extract item columns and purchased items
item_cols = [col for col in df.columns if col.startswith("Item Purchased_")]

def get_purchased_item(row):
    for col in item_cols:
        if row[col] == 1 or row[col] == True:
            return col.replace("Item Purchased_", "")
    return None

df['Purchased_Item'] = df.apply(get_purchased_item, axis=1)

# Define feature columns
feature_cols = ['Age', 'Gender', 'Income', 'Purchase Amount (USD)',
                'NumWebVisitsMonth', 'Review Rating', 'Previous Purchases',
                'Subscription Status', 'Discount Applied', 'Promo Code Used']

color_cols = [col for col in df.columns if col.startswith('Color_')]
size_cols = [col for col in df.columns if col.startswith('Size_')]
season_cols = [col for col in df.columns if col.startswith('Season_')]

all_features = feature_cols + color_cols + size_cols + season_cols

# Create feature matrix
X_features = df[all_features].copy()

# Convert boolean columns to integers
for col in X_features.columns:
    if X_features[col].dtype == 'bool':
        X_features[col] = X_features[col].astype(int)

print(f"‚úÖ Feature matrix created: {X_features.shape}")
print(f"   - Base features: {len(feature_cols)}")
print(f"   - Color features: {len(color_cols)}")
print(f"   - Size features: {len(size_cols)}")
print(f"   - Season features: {len(season_cols)}")

# ============================================================
# 3. MODEL TRAINING
# ============================================================

print("\nü§ñ Step 3: Training models...")

# --- 3.1: Customer Segmentation (K-Means) ---
print("   ‚Üí Training K-Means clustering...")
cluster_features = ['Age', 'Income', 'Purchase Amount (USD)',
                    'NumWebVisitsMonth', 'Previous Purchases', 'Category']

X_cluster = df[cluster_features].copy()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
df['Customer_Segment'] = kmeans.fit_predict(X_scaled)

print(f"   ‚úÖ K-Means: 4 segments created")
print(f"      Segment distribution: {df['Customer_Segment'].value_counts().sort_index().to_dict()}")

# --- 3.2: Customer Similarity Matrix ---
print("   ‚Üí Computing customer similarity matrix...")
customer_similarity = cosine_similarity(X_features)
print(f"   ‚úÖ Customer similarity: {customer_similarity.shape}")

# --- 3.3: Item Profiles and Similarity ---
print("   ‚Üí Building item profiles...")

def create_item_profiles():
    item_profiles = {}
    for item in df['Purchased_Item'].dropna().unique():
        item_customers = df[df['Purchased_Item'] == item]
        profile = item_customers[all_features].mean()
        item_profiles[item] = profile
    return pd.DataFrame(item_profiles).T

item_profiles = create_item_profiles()
item_profiles_filled = item_profiles.fillna(0)

item_similarity_matrix = cosine_similarity(item_profiles_filled)
item_similarity_df = pd.DataFrame(
    item_similarity_matrix,
    index=item_profiles_filled.index,
    columns=item_profiles_filled.index
)

print(f"   ‚úÖ Item similarity: {item_similarity_df.shape}")
print(f"      Unique items: {len(item_similarity_df)}")

# ============================================================
# 4. SAVE MODEL ARTIFACTS
# ============================================================

print("\nüíæ Step 4: Saving model artifacts...")

# --- File 1: recommendation_engine.pkl ---
# Contains core recommendation data
recommendation_engine = {
    'df': df[['Customer ID', 'Purchased_Item', 'Customer_Segment']],
    'X_features': X_features,
    'customer_similarity': customer_similarity,
    'item_similarity_df': item_similarity_df,
    'cluster_features': cluster_features,
    'item_list': sorted(df['Purchased_Item'].dropna().unique().tolist())
}

with open(os.path.join(models_dir, 'recommendation_engine.pkl'), 'wb') as f:
    pickle.dump(recommendation_engine, f)
print("   ‚úÖ models/recommendation_engine.pkl")

# --- File 2: feature_scaler.pkl ---
# Scaler for customer clustering features
joblib.dump(scaler, os.path.join(models_dir, 'feature_scaler.pkl'))
print("   ‚úÖ models/feature_scaler.pkl")

# --- File 3: kmeans_model.pkl ---
# Trained K-Means model
joblib.dump(kmeans, os.path.join(models_dir, 'kmeans_model.pkl'))
print("   ‚úÖ models/kmeans_model.pkl")

# --- File 4: feature_info.pkl ---
# Feature metadata and configuration
feature_info = {
    'all_features': all_features,
    'feature_cols': feature_cols,
    'color_cols': color_cols,
    'size_cols': size_cols,
    'season_cols': season_cols,
    'cluster_features': cluster_features,
    'n_clusters': 4,
    'total_customers': len(df),
    'total_items': len(item_similarity_df),
    'weights': {
        'content': 0.4,
        'segment': 0.4,
        'item_similarity': 0.2
    }
}

joblib.dump(feature_info, os.path.join(models_dir, 'feature_info.pkl'))
print("   ‚úÖ models/feature_info.pkl")

# ============================================================
# 5. VALIDATION
# ============================================================

print("\nüß™ Step 5: Validating saved models...")

# Test loading all files
try:
    test_engine = pickle.load(open(os.path.join(models_dir, 'recommendation_engine.pkl'), 'rb'))
    test_scaler = joblib.load(os.path.join(models_dir, 'feature_scaler.pkl'))
    test_kmeans = joblib.load(os.path.join(models_dir, 'kmeans_model.pkl'))
    test_info = joblib.load(os.path.join(models_dir, 'feature_info.pkl'))

    print("   ‚úÖ All models loaded successfully")
    print(f"   ‚Üí Engine contains {len(test_engine['df'])} customer records")
    print(f"   ‚Üí KMeans has {test_kmeans.n_clusters} clusters")
    print(f"   ‚Üí Feature info tracks {len(test_info['all_features'])} features")

except Exception as e:
    print(f"   ‚ùå Error loading models: {e}")
    exit(1)

# ============================================================
# 6. SUMMARY
# ============================================================

print("\n" + "="*60)
print("‚úÖ TRAINING COMPLETE - MODEL ARTIFACTS SAVED")
print("="*60)
print("\nGenerated files in models/ directory:")
print("  1. recommendation_engine.pkl  (Similarity matrices & customer data)")
print("  2. feature_scaler.pkl         (StandardScaler for clustering)")
print("  3. kmeans_model.pkl           (Trained K-Means model)")
print("  4. feature_info.pkl           (Feature names & metadata)")
print("\nNext steps:")
print("  ‚Üí Create predict.py to load models and generate recommendations")
print("  ‚Üí Create app.py (Flask/FastAPI) for web API")
print("  ‚Üí Build UI (HTML/Gradio/Streamlit)")
print("="*60)